{
  "cells": [
    {
      "cell_type": "raw",
      "id": "abf67a9f",
      "metadata": {},
      "source": [
        "---\n",
        "title: Twitter ETL & Sentiment Analysis\n",
        "date: January 2023\n",
        "categories:\n",
        "  - Python\n",
        "  - Tweepy\n",
        "  - Sentiment Analysis\n",
        "  - Machine Learning\n",
        "  - NLP\n",
        "image: image.jpg\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89059fc8",
      "metadata": {},
      "source": [
        "# Twitter ETL & Sentiment Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "The goal of this post is to demonstrate how we can use Twitter's API & Python's primary natural-language processing (NLP) libraries to easily analyze the overall sentiment of recent tweets on any given subject. \n",
        "\n",
        "Additionally, I'll show how to load this data into a SQL database or S3 bucket on AWS. This type of ETL or data pipeline would likely be necessary in the case that I wanted to continously maintain and analyze a larger list of tweets over time, or in a professional setting where multiple analysts and engineers need to access to the data.\n",
        "\n",
        "The code and analysis below will cover the following topics:\n",
        "- Extracting tweets from the Twitter API using a Python library called Tweepy\n",
        "- Transforming the tweets data into a pandas dataframe for ease of analysis\n",
        "- Cleaning and pre-processing unstructured, unlabeled text data\n",
        "- Loading the cleaned, tweet dataframe into a SQL database or Amazon S3 bucket.\n",
        "- Executing sentiment analysis using libraries like scikit-learn and transformers\n",
        "\n",
        "The source code can be found in my GitHub profile [here]().\n",
        "\n",
        "## *1) Setup*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e651a2a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Load libraries\n",
        "\n",
        "# Twitter\n",
        "import tweepy\n",
        "\n",
        "# Data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# SQL\n",
        "import sqlite3\n",
        "\n",
        "# Regex\n",
        "import re\n",
        "\n",
        "# Environment variables\n",
        "from decouple import config\n",
        "\n",
        "# AWS S3 Upload\n",
        "from datetime import date\n",
        "import boto3\n",
        "import io\n",
        "\n",
        "# Text analytics\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import altair as alt\n",
        "from textblob import TextBlob\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# Configure environment variables\n",
        "BEARER_TOKEN = config('BEARER_TOKEN')\n",
        "AWS_ACCESS_KEY_ID = config('AWS_ACCESS_KEY_ID')\n",
        "AWS_SECRET_ACCESS_KEY = config('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9491db",
      "metadata": {},
      "source": [
        "The last few lines of code above relates to the environment variables I have stored in a separate .env file. These include my Twitter API bearer token as well as my AWS credentials. Using the python-decouple library, I can simply assign this confidential information to new local variables in this file. In managing the bearer token and AWS keys this way, I'm able to make requests to the Twitter API, upload data to AWS, share my source code, and keep my accounts secure.\n",
        "\n",
        "I highly recommend [this article](https://able.bio/rhett/how-to-set-and-get-environment-variables-in-python--274rgt5) for a quick example of how to use python-decouple for this use case.  \n",
        "\n",
        "## *2) Extract*\n",
        "\n",
        "First I will initialize the tweepy client with my bearer_token. A bearer token is a type of authentication access token used for the OAuth 2.0 protocol. In simpler terms, it's a unique string that helps Twitter associate the API requests I make with my Twitter Developer account. \n",
        "\n",
        "Next, I define the query (term to search for on Twitter) as 'USMNT', an acronym for the United States Men's National Soccer Team. US Soccer has undergone some significant internal turmoil since the 2022 World Cup. As a lifelong, die-hard fan of the team, I want to know how people feel about the resignation of two major executives and the investigation into former manager, Greg Berhalter. I also will specify in the query that I only want to see tweets in english and retweets should be excluded.\n",
        "\n",
        "Using the search_recent_tweets Tweepy function, along with my defined twitter_query, I can start extracting relevant tweets. I'll also include the following parameters:\n",
        "- tweet_fields: a list of tweet-specific fields I want be to be returned for each matching tweet object. \n",
        "- user_fields: a list of user-specific fields I want to be returned for each matching tweet object.\n",
        "- max_results: the number of matching tweets to be returned. This number defaults to 10 and is capped at 100.\n",
        "\n",
        "The Tweepy client then makes a HTTP request to the search_recent_tweets endpoint (https://api.twitter.com/2/tweets/search/recent). Assuming I have passed in a valid bearer_token and the parameters I included adhere to the Twitter API & Tweepy documentation, the endpoint will return a list of 100 tweet objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f48a905",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Tweepy client with bearer_token\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Build twitter query - lowercase string, ensure no retweets are included, limit to english\n",
        "twitter_query = \"USMNT -is:retweet lang: en\"\n",
        "\n",
        "# Get tweets and save to new 'tweets' object\n",
        "tweets = client.search_recent_tweets(query=twitter_query,\n",
        "                                    tweet_fields=['context_annotations','created_at', 'public_metrics', 'possibly_sensitive', 'source'],\n",
        "                                     user_fields=['username'],\n",
        "                                     max_results=100)\n",
        "\n",
        "# Extract the actual tweet data from the Tweepy tweets object\n",
        "tweets_data = tweets.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6068b0f2",
      "metadata": {},
      "source": [
        "Next, I'll inspect the tweet attributes I'm interested in for the first 3 tweets. These are:\n",
        "- created_at - time of tweet\n",
        "- text - tweet contents) \n",
        "- public_metrics - public tweet metrics: number of retweets, replies, likes and quotes) \n",
        "- possibly_sensitive - a flag used to identify any potentially sensitive tweets\n",
        "- source - how the tweet was created (phone, desktop, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e17d8103",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through tweets_data and print results\n",
        "for number, each in enumerate(tweets_data[0:3]):\n",
        "    print(f\" \\\n",
        "            Tweet {number} time: {each.created_at}\\n \\\n",
        "            Tweet {number} text: {each.text.strip()}\\n \\\n",
        "            Tweet {number} metrics: {each.public_metrics}\\n \\\n",
        "            Tweet {number} possibily_sensitive?:  {each.possibly_sensitive}\\n \\\n",
        "            Tweet {number} source: {each.source}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0c797e",
      "metadata": {},
      "source": [
        "## *3) Transform*\n",
        "\n",
        "Its important to note that the data returned from the Twitter API comes in JSON format, which is not the same as tabular data.\n",
        "\n",
        "For example, the 'tweets' object contains the following objects:\n",
        "- data - a list of tweets and the attributes I printed above\n",
        "- meta - Metadata such as newest tweet id in the response, oldest tweet id in the response, number of results returned, and the next token/page of tweets.\n",
        "\n",
        "The data cannot be easily analyzed in it's current form (at least not using pandas). Therefore, I'll now execute a series of steps designed to clean and transform the data into a structure that's easier to work with: a dataframe.\n",
        "\n",
        "To do this, I use list comprehension, which allows me to iterate through the list of tweets, saving each tweet value I care about to a new vector. Then I combine the 5 vectors into a pandas dataframe, which is printed below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf91a1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results in dataframe\n",
        "tweet_id = [each[\"id\"] for each in tweets_data]\n",
        "tweets = [each[\"text\"] for each in tweets_data]\n",
        "metrics = [each[\"public_metrics\"] for each in tweets_data] \n",
        "sensitive = [each[\"possibly_sensitive\"] for each in tweets_data] \n",
        "source = [each[\"source\"] for each in tweets_data] \n",
        "\n",
        "# Save to df\n",
        "df = pd.DataFrame({\n",
        "    \"tweet_id\":tweet_id,\n",
        "    \"tweets\":tweets,\n",
        "    \"metrics\":metrics,\n",
        "    \"sensitive\":sensitive,\n",
        "    \"source\":source\n",
        "      \n",
        "})\n",
        "\n",
        "# Inspect\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e5642d",
      "metadata": {},
      "source": [
        "This isn't quite what I need, because the public_metrics values were stored as a dictionary and therefore not in separate columns in the dataframe. Below, I'll update the dataframe to fix this issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6412106",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate metrics dictionary to columns\n",
        "metrics_cols = df[\"metrics\"].apply(pd.Series)\n",
        "\n",
        "# Add metrics columns to df\n",
        "df = df.join(metrics_cols)\n",
        "\n",
        "# Drop original metrics column\n",
        "df = df.drop(columns=[\"metrics\"])\n",
        "\n",
        "# Inspect\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5c3528",
      "metadata": {},
      "source": [
        "That looks better. I'll now proceed with cleaning the actual text data to remove unnecessary characters contained in the tweet.\n",
        "\n",
        "First, I create a function utilizing the re (regex) library to remove the irrelevant content from the tweet (e.g. twitter handles and mentions, included links, non-letter characters). I then apply this function to each value in the \"tweets\" column of the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feda0ef9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean tweets\n",
        "def process_tweets(t):\n",
        "    t = t.lower()\n",
        "    t = re.sub(\"rt \", \"\", t)            # remove \"rt\", a term automatically left in if the tweet is a retweet\n",
        "    t = re.sub(\"@[A-Za-z0-9]+\", \"\", t) # remove handles\n",
        "    t = re.sub(\"#[A-Za-z0-9]+\", \"\", t) # remove hashtags\n",
        "    t = re.sub(r\"http\\S+\", \"\", t)      # remove links (anything that doesn't have a space after http)\n",
        "    t = re.sub(r\"www.\\S+\", \"\", t)      # remove links\n",
        "    t = re.sub(\"[()!?]\", \"\", t)        # remove punctuation\n",
        "    t = re.sub(\"\\[.*?\\]\", \"\", t)       # remove puncutation\n",
        "    t = re.sub(\"[^a-z]\", \" \", t)       # remove anything that is not a letter\n",
        "    return t\n",
        "\n",
        "# Apply function to tweets\n",
        "df[\"tweets\"] = df[\"tweets\"].apply(process_tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5095428",
      "metadata": {},
      "source": [
        "Now we can look at the cleaned text data, sorted by number of retweets to get a better sense of what people are tweeting about our topi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e007d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect tweets with most retweets\n",
        "retweets = df.sort_values(by=[\"retweet_count\"], ascending=False)\n",
        "\n",
        "# Print 10 top most retweeted tweets\n",
        "retweets[[\"tweets\", \"retweet_count\"]][0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddeb19cf",
      "metadata": {},
      "source": [
        "## *4) Load*\n",
        "### SQL Database\n",
        "\n",
        "Next, I use SQLite to create a local SQL database (i.e. on my computer in the same folder as this file). I'll also create a table called \"queried_tweets.db\" to store the tweets data contained in pandas dataframe \"df\". \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f12020",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create db and save as a SQLlite table\n",
        "%reload_ext sql\n",
        "%sql sqlite:///queried_tweets.db\n",
        "c = sqlite3.connect(\"queried_tweets.db\")\n",
        "cur = c.cursor()\n",
        "df.to_sql(\"tweets_table\", con=c, if_exists=\"replace\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2992403",
      "metadata": {},
      "source": [
        "Here I test to ensure that I can access the new SQL database using SQL queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680a8861",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%sql\n",
        "SELECT * \n",
        "FROM tweets_table\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c179ec28",
      "metadata": {},
      "source": [
        "This isn't especially valuable right now as I also have the pandas dataframe loaded. However, if I had thousands or millions of tweets to work with and a team of data analysts, this would allow us to leverage more complex SQL queries to efficiently extract different subsets of data from a centralized database.\n",
        "\n",
        "I also could modify the query so that instead of \"replacing\" the table, I simply add more records instead. This would be useful if we wanted to measure sentiment over time, stream tweets in real-time, or were interested in analysis required a larger number of tweets.\n",
        "\n",
        "\n",
        "### AWS - S3 Bucket\n",
        "\n",
        "Additionally, a more-modern solution would be to upload this data to a cloud storage provider like AWS. As an example, I'll demonstrate how to upload the pandas dataframe containing the tweets data to an S3 bucket (as a csv). I could also combine the SQL & AWS loading methods to create and add to a SQL database hosted on AWS. For now though, I'll stick with a S3 bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c909bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set today equal today's date\n",
        "today = date.today()\n",
        "\n",
        "# Create file name\n",
        "tweet_file = \"clean_tweets_\" + today.strftime('%m-%d-%Y') + \".csv\"\n",
        "\n",
        "# Specify bucket & region\n",
        "s3_bucket_name = 'twitter-api-project'\n",
        "aws_region_name = 'us-east-1'\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3', \n",
        "                        region_name = aws_region_name,\n",
        "                        aws_access_key_id = AWS_ACCESS_KEY_ID,\n",
        "                        aws_secret_access_key = AWS_SECRET_ACCESS_KEY)\n",
        "\n",
        "# Write dataframe to S3\n",
        "with io.StringIO() as csv_buffer:\n",
        "    \n",
        "    # Convert to CSV\n",
        "    df.to_csv(csv_buffer, index = False)\n",
        "    \n",
        "    # Upload file to bucket\n",
        "    response = s3_client.put_object (\n",
        "        Bucket = s3_bucket_name, \n",
        "        Key = tweet_file, \n",
        "        Body = csv_buffer.getvalue()\n",
        "        )\n",
        "    \n",
        "    # Get response\n",
        "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
        "    \n",
        "    # Print response for confirmation\n",
        "    if status == 200:\n",
        "        print(f\"Successful S3 put_object response. Status - {status}\")\n",
        "    \n",
        "    else:\n",
        "        print(f\"Unsuccessful S3 put_object response. Status - {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a63c63",
      "metadata": {},
      "source": [
        "As the expected 200 response shows, I have now successfully uploaded the tweets data to S3. \n",
        "\n",
        "Here's a screenshot of my AWS account:\n",
        "\n",
        "![Extracted tweet data now on S3](s3-screenshot.png)\n",
        "\n",
        "[Download the CSV file here](clean_tweets_01-27-2023.csv) to see for yourself\n",
        "\n",
        "## *5) Analytics*\n",
        "\n",
        "### Wordcloud\n",
        "\n",
        "For the final component of this project, I will now execute various methods of sentiment analysis to demonstrate the how to analyze unlabeled, unstructured text data.\n",
        "\n",
        "A simple form of text analysis is to display a simple wordcloud, which shows a visual representation of the most commons words from the processed tweets column.\n",
        "\n",
        "Note that prior to creating the wordcloud, I use the stopwords library to set the list of words to exclude.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cac6a60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set stopwords\n",
        "#nltk.download('stopwords')\n",
        "sw = stopwords.words(\"english\")\n",
        "\n",
        "# Append non-useful words to stopwords list\n",
        "appended_stopwords = [\"usmnt\", \"would\", \"u\", \"soccer\", \"one\", \"us\", \"get\"]\n",
        "sw.extend(appended_stopwords)\n",
        "\n",
        "# Build wordcloud of word freqs in query\n",
        "wc = WordCloud(stopwords=sw).generate(\" \".join(df[\"tweets\"]))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(wc)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af52469",
      "metadata": {},
      "source": [
        "### Vectorize\n",
        "\n",
        "The vectorizer below transforms the column of clean tweets into a vector of most common words. I first convert the newly vectorized column into a dataframe, then transpose it into a term-tweet matrix. Finally, I add a column with a count of how many times each term was mentioned in the list of tweets, then print the sorted results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61352fb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize vectorizer\n",
        "v = CountVectorizer(stop_words=sw)\n",
        "\n",
        "# Vectorize tweets column\n",
        "v_tweets = v.fit_transform(df[\"tweets\"])\n",
        "\n",
        "# Convert vectorized tweets into dataframe\n",
        "v_tweets_df = pd.DataFrame(v_tweets.toarray(),\n",
        "                          columns=v.get_feature_names_out(),\n",
        "                          index=df[\"tweet_id\"])\n",
        "\n",
        "# Transpose it to get a term-tweet matrix\n",
        "v_tweets_df = v_tweets_df.T\n",
        "\n",
        "# Calculate frequencies of tweets for each term (row)\n",
        "v_tweets_df[\"freq\"] = v_tweets_df.sum(axis=1)\n",
        "\n",
        "# Inspect new tweets\n",
        "v_tweets_df[\"freq\"].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a842bd7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make term into a column, select freq and term columns \n",
        "v_tweets_df[\"term\"] = v_tweets_df.index\n",
        "v_tweets_df = v_tweets_df[[\"freq\", \"term\"]]\n",
        "\n",
        "# Keep top 30 most frequent terms\n",
        "v_top = v_tweets_df.sort_values(by=\"freq\", ascending=False)[:30]\n",
        "\n",
        "# Plot terms by frequency\n",
        "alt.Chart(v_top).mark_bar().encode(alt.X(\"term\", sort=\"-y\"),\n",
        "                                    y=\"freq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49ef2330",
      "metadata": {},
      "source": [
        "### TextBlob Sentiment Analysis\n",
        "\n",
        "In the code below, I leverage the TextBlob library to calculate two types of sentiment scores for each tweet: \n",
        "- Polarity (positive = 1, negative = -1) \n",
        "- Subjectivity score (completely subjective = 1, completely objective = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81a9ad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize empty vector to score polarity & subjectivity scores\n",
        "polarity = []\n",
        "subjectivity = []\n",
        "\n",
        "# Loop through and extract sentiment/polarity of each tweet\n",
        "for i in df['tweets']:\n",
        "\n",
        "    # Get sample polarity & subjectivity scores\n",
        "    sample = TextBlob(i)\n",
        "    \n",
        "    # Append scores to associated vecotrs\n",
        "    polarity.append(sample.sentiment[0])\n",
        "    subjectivity.append(sample.sentiment[1])\n",
        "    \n",
        "    \n",
        "# Add columns to the dataframe\n",
        "df[\"polarity\"] = polarity\n",
        "df[\"subjectivity\"] = subjectivity\n",
        "\n",
        "# Inspect results\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63fa618",
      "metadata": {},
      "source": [
        "Additionally, I can now create custom sentiment labels (and therefore sentiment categories for my set of tweets) based on the polarity score. I do this by marking any score below 0 as negative and any score as above 0 as positive. Alternative scoring calculations could be used here as well depending (e.g. Neutral could extend from -0.10 to 0.10 or -0.05 to 0.05). This might be preferrable if you only wanted to see highly negative tweets related to a new feature or open defect that has been discovered with a product. Another option would be to create more labels like \"Strongly positive\", \"Strongly negative\", etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bede09e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sentiment groups based on polarity scores\n",
        "df[\"pol_group\"] = np.where(df[\"polarity\"] > 0, \"Positive\",\n",
        "                          np.where(df[\"polarity\"] < 0, \"Negative\", \"Neutral\"))\n",
        "\n",
        "\n",
        "# Inspect the results more closely\n",
        "df_sent = df[[\"tweets\", \"pol_group\", \"polarity\", \"subjectivity\"]]\n",
        "df_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68e23b2",
      "metadata": {},
      "source": [
        "### Transformers Sentiment Analysis\n",
        "\n",
        "Using sentiment labeling for TextBlob not always the most accurate method (as we can see). I'll now try an alternative approach using the transformers library.\n",
        "\n",
        "Transformers uses pre-trained models from the most popular Python deep learning/machine learning libraries: Tensorflow, PyTorch, and Jax. I can apply these robust, well-developed models to my dataset of tweets via a process called transfer learning. \n",
        "\n",
        "For this example, I will apply the model to an unlabeled sentiment analysis task (i.e. calculate the sentiment of the tweets contained in my dataframe). I'll use distilBERT base uncased finetuned SST-2 model. This is one of the most widely-used, most accurate, and fastest sentiment analysis models available for this type of work.\n",
        "\n",
        "More information regarding the model can be found on Hugging Face [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29f6106",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7893cc41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create empty vector for sentiment scores\n",
        "sent_label = []\n",
        "sent_confidence = []\n",
        "\n",
        "# Apply model and extract sentiment labels/scores\n",
        "for i in df_sent[\"tweets\"]:\n",
        "    sample = model(i)\n",
        "    sent_label.append(sample[0][\"label\"])\n",
        "    sent_confidence.append(sample[0][\"score\"])\n",
        "\n",
        "# Append columns to df\n",
        "df_sent[\"sent_label\"] = sent_label\n",
        "df_sent[\"sent_confidence\"] = sent_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e11de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect results (different rows this time)\n",
        "df_sent.iloc[40:60]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e85fcb",
      "metadata": {},
      "source": [
        "The labels appear to be a bit more accurate, although not perfect given the irregular ways in which people communicate via Twitter. The text is not nearly as clean or straightforward as a product review or an industry report, so it makes since that the model would have a more difficult time parsing and correcting classifying each tweet.\n",
        "\n",
        "As a last step, I'll create a final iteration of the dataframe and plot the results to make reviewing the sentiment analysis scores a bit easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd6cb41",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.crosstab(df_sent[\"sent_label\"], columns=\"index\", normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec243f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop columns I don't need \n",
        "df_clean = df_sent.drop(columns=[\"pol_group\", \"subjectivity\"])\n",
        "\n",
        "# Sort and color by TextBlob score\n",
        "df_clean.sort_values(by=\"polarity\").style.background_gradient(cmap=\"coolwarm\", \n",
        "                                                              subset = [\"polarity\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ae93265",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "The steps I have walked through show how easy it can be to pull unstructured text data from an API, clean and process the data, transform it into a form that can be analyzed, then execute several variations of unlabeled sentiment analysis. Thanks to the open-source nature of web development, the libraries I leveraged allowed me to quickly perform these operations with relatively few lines of code.\n",
        "\n",
        "The primary challenge in this work, however, is not running the code to apply sentiment scores or create visualizations, but in actually designing the data pipeline from the Twitter API to the models you want to run. As we can observe from some of the less accurate sentiment scores, text data (especially Twitter text data) is quite messy and difficult to analyze cleanly.\n",
        "\n",
        "If done correctly however, this type of analysis offers incredible insights. Inquiries to customer service teams could be automatically labeled, grouped and prioritized according to their sentiment and topic-modeling results (another similar method of text analysis). Similarly, product and marketing teams could use these techniques to better understand their customers and help drive product strategy (i.e. what do we build next, what do our customers want to see). \n",
        "\n",
        "In my next post on this topic, I plan to bundle sentiment analysis code into a streamlit app. This would allow anyone to enter a term and see the resulting twitter sentiment analysis based on the most recent tweets. \n",
        "\n",
        "Thanks for reading!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (main, Jan 11 2023, 09:18:20) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "cedc0c42954c8e445df387472eea2c6a1b4378f99edb5b9d095638347ec4a571"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
